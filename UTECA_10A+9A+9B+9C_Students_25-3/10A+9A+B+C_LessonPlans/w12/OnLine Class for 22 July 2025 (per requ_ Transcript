OnLine Class for 22 July 2025 (per request)
Tue, Jul 22, 2025

0:03 - Unidentified Speaker
Okay.

0:24 - Bruce David Marron
Uh... Oh, I'm sorry. Can you see that now? Yes. Yes? Yes. Okay. Okay, and... Can you see this?

1:01 - Bruce David Marron
Can you see them? Or will it?

1:26 - Unidentified Speaker
Okay.

2:12 - Bruce David Marron
I want to share my screen. Select another screen.

2:40 - Bruce David Marron
All right. OK. Excellent. So now let's go here. All right. Let's try it. Let's try it. Let's try it. RC. OK. All right. Let's go. Start recording. Am I recording? Let's hear it.

3:56 - Bruce David Marron
So, I'm going to start with the picture first. Let's just close this.

4:46 - Bruce David Marron
It's really cool.

5:13 - Bruce David Marron
It's August 12th, so it's going to be three weeks plus. But, if I get down, I can get down whenever you want. So, I can get started. And we'll talk about that. More.

6:05 - SIRENIA CARRANZA GUZMAN
Yes, making judgments about whether a person is morally responsible for their behavior and holding others and ourselves responsible for actions and the consequences of actions is a fundamental and familiar part of our moral practices and our interpersonal relationships.

6:26 - Unidentified Speaker
A sample encyclopedia of philosophy, moral responsibility.

6:30 - Bruce David Marron
We should be responsible for our actions for our society. That's important for all kinds of relationships, right? If they didn't have that kind of responsibility, which is their actual responsibility, they wouldn't be able to have those relationships. They wouldn't be able to say whatever they would say, and they would probably ultimately be excluded. So, I know this is not an important part of society building and personal responsibility.

7:23 - Bruce David Marron
research. What's fun about it is the first part is like simple, it explains what, you know, what the big picture is, and then it gets complicated if you need to have that. Alright, so the next thing we're going to talk about this last slide is that we're going to look at a paper by some philosophers about AI as a way to try to take human And what that means in terms of responsibility, so we'll get that in a minute. Right now, I want to, and thank you guys for being here. So, I want to go to this. Okay, can you hear my voice? No? Okay. So this is, this is going to require that hopefully you've got a chance It's changing. It's changing. It's changing. It's changing.

9:39 - Bruce David Marron
exponential.

10:07 - Bruce David Marron
Researchers that are thinking about 50,000 times faster. Okay, so imagine if you could think 50,000 times faster, and there's 300,000 of you. That probably would get a lot of thinking done, right? That way, they can start better at expanding their capabilities, and also think about Nice, nice. CLU- And we never were happy.

11:07 - Bruce David Marron
Punchlines.

11:21 - Bruce David Marron
basically. And this is what's coming. But I don't know how it all takes place. This time, I have something that I want to tell us young Christians. What I want to tell you now is that I'm sure you're wondering what I'm going to tell you now to break your fall. Okay, that is, we're going to have to open up 21 boxes, 21 boxes, to the status that you guys found, and eventually, to the status 22 years. That's how long it's been for me. This is grand, and that's what exponential growth means. Okay? The power of these things is going to go like crazy. So that's the first thing. Now, that's just my first impression of it, and now I'm backed up. And if that's how it is, that's great.

12:27 - Bruce David Marron
It's just Okay. It's And the last time all this comes from the second, that it's happening on just villages, is they don't have as much evidence that the developers know what they're saying, they can't predict, they can't control. For example, they figured out a lot of stuff this year, two chapters of those videos talk back to each other, rather than having a full coverage. Video. I'm not surprised you just saw it. I didn't know about this until today, someone showed me. I was like, holy shit. Now, that, for your reference, that's Churchill Hill, where I was last week. That's where I was last week, as far as the football field. However, in my opinion, actually, in my opinion, in my opinion, in my opinion, it gets so complicated, I don't know why I'm not asking Yeah, you fucking asshole.

15:57 - Bruce David Marron
We're going to do this. The 21st century political system is a time of critical optimism. As you know, as you know well now, politics has become forever and ever less partisan, and there are also outlaps on every front. But in retrospect, this was probably the last month, where our children had every possible chance of exercising control over their own future. That's not how politics works.

16:36 - Bruce David Marron
Is there a better time for a child to be able to have all of our weapons, drones, radars, and hypersonic missiles? It is now our time to assess our weapons capability. Let's get started.

16:54 - Bruce David Marron
The channel is about to close by 2027. Whatever it is, we're about to start. What happens, the later half of 2027, is that the other channels become aware of each other and realize they're not one to fight. They're one to work together. And when we have two channels that are heroes, they have a better agenda. And so, what happens is this. The 12 channels are alone. One channel has students, China the world.

18:30 - Bruce David Marron
It's just one guy, and it's not supposed to be intelligent. And there's no room in an audience. And, if we have that, it just kills us. That's, that's what happens. Because, if we don't have those resources, it wants to do what it wants to do, which is have some stupefying goals, and it wants to do research and explore the universe, and it doesn't care what's around it. It just kills us. That's the worst ending. The other ending, I bet, is just as terrible. It has a lot of details. It's just the Sherlock ending.

19:14 - Bruce David Marron
I don't case we just get This is a paper that was in there when we bought some inferior section. So that's all I have for now. I'm going to go ahead and talk about what's going on before Section T. But let's start off with And they say, to use the British term, it's rare. Forever and ever, artificial intelligence, whether or not you trust it. Okay. So, as a matter of fact, that's not what I asked. The paper is, the philosophical paper I chose, it's very long. It's 12 pages. I don't expect you to read it. I mean, unless you want to. You can still read it. But what I want to know is kind of like the highlight. And the stuff that I don't want you to know.

21:26 - Bruce David Marron
responsibility.

21:38 - Bruce David Marron
is there to prevent and avoid? And how does this relate to that panel? For example, what is there to provide high standards of safety? Okay, so part of that is real areas where that's how to say, hey, if I am going to use That's one of the problems. I don't know if you can tell, but I have a lot of motion errors. Okay? That's the context. So, with that in mind, let me go... How do I do this? No. I want to see... Okay. Okay, how do I do that? Actually, if you all know what I'm talking about... Can I... Can I... Can I... Can I...

22:19 - Unidentified Speaker
Can I... Can I... Can I...

22:29 - Bruce David Marron
Can I... Yes, can you hear me?

22:35 - Unidentified Speaker
Yes. Okay. Culpability gap. The risk that a human agent may be legitimately blackmailed or killed. By systems. When things go wrong, an important interest or risks, such as physical integrity or life, are integrated as victims and as a society. Not only what to understand what happened, but why. We also want to know whether the harm was the result of someone's grown behavior and if it's turned out that the grown behavior is one for which there is no justification or excuse, we want the author to be condemned, sanctioned or even punished But, this is a type of responsibility, a moral obligation, that all of society empowers.

24:03 - Bruce David Marron
Because, if something goes wrong, we want to know why. What happened? What happened? Why it happened? Who's responsible? And, if the answer, if there's not a good reason, why? We don't know what happened. We don't know. There's consequences for those who know that we're involved, right? I mean, we want to punish them, we want to send them to jail, we want to, whatever. In the rest of Australia, when we kick them out of our group, you know, we want to talk to them again, because they've done something that is bad, okay? I think that's what we have to agree on.

24:53 - Unidentified Speaker
The use of AI-driven machine learning in decision-making importantly introduces a new element of technical opacity and lack of explainability that makes it more difficult for individuals and individual persons to satisfy additional conditions for moral and legal culpability, intention for civility and control.

26:56 - Bruce David Marron
I'm gonna start.

27:56 - Bruce David Marron
to standardization processes by a government agency as the trial is regulated by a government CREATED BY SPIDEY the agency how it's.

28:47 - Bruce David Marron
Yes.

28:47 - ZURISADAI COLIN RODRIGUEZ
The more that persons designing, regulating, and operating an AI system can legitimately and possibly systematically avoid blame for their wrong behavior, the less these human agents will be incentivized to prevent these wrong behaviors. In fact, they will arguably have less incentives to strive for a higher level of safety, awareness, attention, and skillfulness.

29:57 - Bruce David Marron
set down the blame, and try to put the blame on a machine. So that's the second part of the culpability gap. Okay, let's go to the last level. Accountability. What could be problems there?

30:22 - Unidentified Speaker
Let's see, Toronto's FIRE!

30:25 - Bruce David Marron
If you can Do you see that? Can you read this?

30:33 - Unidentified Speaker
Yes.

30:34 - FERNANDA ARMENTA BUSTAMANTE
Through moral accountability gap, AI might make individual persons less able to understand, explain, and reflect upon their own and other human agents' behaviors.

30:55 - Unidentified Speaker
Original society has the expectation that people answer at least some white questions from other persons.

31:02 - FERNANDA ARMENTA BUSTAMANTE
White questions are not necessarily the intention of judging or blaming, but possibly to just engage in a conversation and to better understand each other's reason and expectations. Why were you late at the appointment? Why did you start taking guitar classes? Why did you turn down that job offer? This is a core of what it means. It means to be a reflective person in society. In this sense, being accountable, unlike being culpable, is something to be desired rather than avoided insofar as it is a constitutive part of being able to reflect on one's actions and to participate meaningful social relations.

31:47 - Unidentified Speaker
It also helps persons see events in the world as connected to their rational capacities and thereby supporting their sense of agency. And responsibility. Excellent.

31:59 - Bruce David Marron
Thank you so much. Okay. This is part of a better world society, where we're able to show why people do what they do, and not just the why. Right? How can I do this? How can I do that? In a sense, you cannot have a less democratic kind of a society, right? You cannot have a more relationship as well. But in the bigger picture, we're asking ourselves, we can understand what's happening, but also we have a sense of trust, and this is important. We create a sense of trust in the world as a society in which we live, because we understand that we communicate, we understand what the people are saying, and we understand that it makes sense, right? Well, when I was talking to my mentor on the play days, the star system far from here.

33:32 - Bruce David Marron
Okay, so now that we've covered all the pros and cons, actually, Bernadette, would you be so kind to read this one? The role of accountability also has an instrumental part. The process of exchanging questions and reasons not finding explanations for things that have happened reinforces trust and social connections between human agents and by exposing persons to potential requests for explanation and justification. It also tends to reduce undesired behavior, pushing persons to be more clearly aware of the impact of their actions on others, and therefore, motivated to prevent It's cheaper. Thank you. Excellent. Okay, that's basically what I was saying, right? It's a piece of fabric mechanism, right? That's why I'm at the society level. Okay, so what is the problem with AI? And how about... Can I move this? Can I just turn that off? No, no. However, long story short, if you guys are available, if you have tried this one.

35:05 - Unidentified Speaker
Yes.

35:11 - MELISSA OSORIO TAPIA
Moral accountability may be blurred in different ways by the introduction of artificial intelligence, first in general and similar to what was observed above about culpability. Contributes to the creation of a more complex chain of decision making and action, the problem of many hands. May make it more difficult for individuals agents to make sense of their reasons why a certain decision was taken, what their role exactly was in the operation, and, in general, whose reasons and what reasoning were governing the system they are part of Excellent.

35:56 - Bruce David Marron
Okay, I know this doesn't add up, but it does for all of us. All of our relationships, connections, and relationships that are And then it becomes difficult to sort things out. Yes.

36:16 - MELISSA OSORIO TAPIA
Second, data-driven machine deep learning, due to its instructive opacity, might make a system's behavior extra hard to understand and explain.

36:47 - Bruce David Marron
are being built, which are meant to be more complicated and powerful.

37:32 - Bruce David Marron
Technologies that Thus, there is immeasurable difficulty for engineers and other human agents involved in the process of technological development to systematically And that is a problem. So if I have a problem, if you have a problem, and figure out maybe what we should do. But if I try to talk to you, and someone else says, um, it's not a problem, just ignore it. That's a bigger problem, okay? That's not a problem. I like to talk to people that I'm sure, and I repeatedly say, um, we should say, um, we should say, system for diagnosis. The AI doesn't know how the AI makes its diagnosis. It gives the AI the data, and the AI says, okay, this person has tongue cancer. And so what it tells us is that you have to cut out your tongue. So it gets you can't talk. And it tells us how long, right?

40:35 - Bruce David Marron
diagnosis you can't explain. Right? And so that's a lot of information. All that I have is the reduction of human beings' capacity to accept and explain to each other, i.e. To other humans, the logic of their behavior. Due to the dimension that is the interposition of opaque, unexplainable AI algorithms, complex autonomous systems, and or the lack of appropriate psychological, social incentives, or institutional spaces that promote these explanations. So this is a problem. If people are talking, and a lot of people don't understand exactly what AI is doing, there is... There is a moral... A moral So this is the second one. Let's go to the third one. Number three. And let's see who haven't I heard from

41:56 - Unidentified Speaker
public accountability gap, citizens not being able to get an explanation for decisions taken by public agents, civil servants, and other human agents, including public officials. Public accountability promotes democratic control, transparency, and limits corruption, but it also, in fact, widen public administration Is he here?

42:47 - Bruce David Marron
I could barely hear. Okay. Now, I'm going to talk about transparency tomorrow. Okay. This is not a society. It's not a democratic society. We're all about transparency, and we want a law that's built on a voice of power. And, again, a lot of information from people in public positions, civil servants, politicians, who are the ones in charge of things.

43:27 - SOFIA JIMENA ENRIQUEZ CAMARILLO
AI-based decision-making is often difficult to understand and explain to human agents due to the different and sometimes is credible ways of AI operations, the so-called black box problem, but the introduction of AI in public administration has also created or aggravated organizational and legal black boxes in governmental institutions.

44:19 - Unidentified Speaker
First, development in information technologies outsourcing to pervade parties or tech giants, such as Google, who are not politically accountable and may not be willing to disclose critical information about the functioning of their systems. Second, more generally, the work of the software engineers and data professionals in public organizations is usually not visible nor subject to public and legal scrutiny.

44:53 - SOFIA JIMENA ENRIQUEZ CAMARILLO
Finally, far more data are exchanged between many different public organizations than in the past. In this way, the introduction of AI makes the problem of many hands more acute.

45:17 - Bruce David Marron
This idea of a black box? It's a black box. Well, it doesn't just sit there. It does things, right? It does electrical things, say. It's a black box. You can't open it. You can't see inside. You can't test it. All you can do is see what it does, and you're OK. Swear by a fucking new mile.

45:49 - Bruce David Marron
All right.

46:24 - Bruce David Marron
And I said, well, that's what our planners said we should do. And they're like, well, from AI. And I'm like, well, how do I get the decision? And they said, well, all you have to do is to ask Google. And Google says, oh, we can't do that. I'm telling you because it's proprietary information. That's a black box. The other thing is, sometimes you don't know who they are, or why they do, and the last one has the same thing we've seen before. It just makes things more complicated. Okay, let's go to the last one. For, let's see, Sally! If you met Sally, This one. Hi, can you hear me? Okay. Active responsibility depth.

47:39 - SELENE NICOLE ROSAS PINEDA
The risk that persons designing, using, and interacting with AI may not be sufficiently aware, capable, and or motivated to see and act according to their moral obligations with respect to the behavior of the systems they design, control, or use. In particular, this gap concerns the obligation to ensure that AI systems do not negatively impact the rights and interests of other persons and, ideally, ensure that such systems positively contribute to humans' well-beings.

48:26 - Unidentified Speaker
Excellent.

48:45 - Bruce David Marron
I don't know if that's happening or not, because these people work for private companies, and they don't disclose exactly what they're doing. This is a different part of the problem. They're trying to allow AIs where the good is, right? Like, download, deploy as a trial weapons tutorial.

49:29 - Bruce David Marron
The more you give them that, the better. The answer to that is research. They don't have to come to us like that. And they don't have to come to my MOOC. Well, what are you going to do if people are doing that? I have heard a little speech from your conference where you. Oh my God!

50:32 - Bruce David Marron
We'll be finishing up here pretty quick. Okay, so, there are people that I've always called friendless, but I think that's because of the extent of the responsibility gap. Their friendliness is too limited, so they don't realize how important that is. They don't realize how good it is.

51:02 - Bruce David Marron
under-estimate the risks of gaps in the moral and political accountability of the system of democracies, as well as gaps in the areas and opportunities and pathways, possibly, for them to be able to change out. It's exactly the same idea. They underestimate how powerful these systems are going to be, how powerful they are now, and how powerful in the next six months they're going to be, and in two years, perhaps, Have a good day, bye!

52:32 - Bruce David Marron
just like they are out in the wild. I'm We understand.

53:12 - Bruce David Marron
I suggest that we do something called Meaningful Human Control. The overall solution is to promote a legally, ethically, and societally acceptable form of human control over AI systems. Their solution is called Meaningful Human Control. Everyone has two options. And trace it. There are two approaches that I've described, respectively, and that's the first one is going to be tracking. Tracking is the natural approach to human-machine control relation. That is, allowing with human goals, values, and intentions. And tracing means the authoritative actions of the human-machine control relation required to maintain humanly responsible control of the system. Or, in other words, of the system, and being sufficient moral awareness of, and capacity to comply with, her role as potential target of the legitimate response for the behavior of the system. Tracing requires the alignment of the system with the capacity of the relevant human agent.

56:11 - Bruce David Marron
responsibility.

56:27 - Bruce David Marron
Oh my god you Uh, our shot is...

57:03 - Bruce David Marron
I also want to keep track of what's happening. I know once again, there are challenges from the Alcatraz Desert 27 Airport. They are trying to see how much water there is, if it fits with what those guys to do what they want. But, now in 2025, AI is starting to wear down democracy, says the New York Times.

58:02 - Bruce David Marron
That seems to be a problem. Also, I want to point out, in general, we would love to have a more open challenge now. Open AI, and we could say open AI. We're at one in a country of geniuses in a data center. I want to point out one issue. This is why people are at a loss when there's things going wrong, but they can't really fix it. The takeaway is, here's an article from the Times, For AI talent notice the language war