							LESSON PLAN  (4 hrs -40 min)

Docente:		Bruce D. Marron
Ciclo: 			25-3
Materia: 		Ingl√©s X
Curso:			IT1044
Licenciatura: 	Interpretaci√≥n y Traducci√≥n
Grupo:			10A

Docente:		Bruce D. Marron
Ciclo: 			25-3
Materia: 		Ingl√©s IX
Curso:			IT0939
Licenciatura: 	Interpretaci√≥n y Traducci√≥n
Grupo:			9A+B+C


#####################

Final Exam [Due: 12 Aug 2025]

The final exam will be a take-home exam and will be due in hard copy format on 12 Aug 2025 during your regularly scheduled class at UTECA. The attached document provides all of the necessary information and detail to successfully complete the final exam. 

Good luck and thank you for a wonderful term!

----------------------------------------------------------------------------


HW_12 [Due: 30 Jul 2025] -- LAST ONE!	<== 9A+B+C
HW_12 [Due: 31 Jul 2025] -- LAST ONE!  <== 10A


--- Task 1
Skim the paper 05a_deSio-Mecacci_ResponsibilityGaps_AI.pdf and read 05b_EXCERPTS_ResponsibilityGaps_AI.txt. Answer the following questions regarding AI and responsibility:

1. What is meant by the term, "responsibility gap" with regards to AI?
2. Define the Culpability Gap and briefly explain why it matters.
3. Define the Moral Accountability Gap and briefly explain why it matters.
4. Define the Public Accountability Gap and briefly explain why it matters.
5. Define the Active Responsibility Gap and briefly explain why it matters.
6. What is the problem with "deflationist" solutions to AI responsibility gaps?
7. What is the problem with ‚Äútechnical solutionism‚Äù with respect to to AI responsibility gaps?
8. What is the problem with ‚Äúlegal solutionism‚Äù with respect to to AI responsibility gaps?
9. Briefly describe the authors' integrated solution to AI responsibility known as Meaningful Human Control (MHC).


--- Task 2
Read Chapter 16, Chapter 17, Chapter 18, Chapter 19 of "Bless Me, Ultima." Select and answer just four (4) questions from the list of questions for each chapter. The questions can be found in the document, "UTECA_StudyGuide_Bless-Me-Ultima.txt. Be sure to format your answers correctly (see Marron_StyleGuide_25-3.pdf).



#############################################
all students--
CEO Rodrigo Vargas
Pablo Cruz ‚Äì Actor de "Sin querer queriendo", "Las Viudas de los Jueves"
Jos√© Ram√≥n Zavala ‚Äì Conductor de "Autos y m√°s"
Marketeatro
Coahuila 105, Roma Nte., Cuauht√©moc, 06700 Ciudad de M√©xico, CDMX
Se suspender√°n clases el martes 22 de julio de las 10AM hasta las 2PM. 
Clases que se den antes y despu√©s de ese horario se impartir√°n de forma normal.


all INT 9no---
üìÖ Martes 22 de julio
üïî 17:00 hrs
Pablo Uranga, int√©rprete y traductor profesional
trabaja en el sistema migratorio estadounidense
 



#######
Today
#######

---- Open


"Making judgments about whether a person is morally responsible for their behavior, and holding others and ourselves responsible for actions and the consequences of actions, is a fundamental and familiar part of our moral practices and our interpersonal relationships."
		--- Stanford Encyclopedia of Philosophy, Moral Responsibility
		(https://plato.stanford.edu/entries/moral-responsibility/ accessed on 21 June 2025)
		



%%%%%%%%%%%%%	Content	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

--- Review



--- New

** AI 2027
What do you think????

The AI alignment problem (in the story) is a moral accountability problem.


** AI and Responsibility
Filippo Santoni de Sio and Giulio Mecacci in Philosophy & Technology (2021) 34:1057‚Äì1084
"Four Responsibility Gaps with Artificial Intelligence: Why they Matter and How to Address them"

---- The Problem Gaps in Responsibility with AI -------------------------------------------
Gaps appear in i) passive responsibility or the moral and legal consequences (human) agents must face in case something goes wrong. And gaps appear in ii) active responsibility or gaps in the goals, values, and legal norms that (human) agents (such as engineers) are supposed to promote and comply with, as well as the consequences they need to prevent and avoid (eg, the legal duty to provide high standards of safety).
	
1. Culpability Gap -- the risk that no human agent might be legitimately blamed or held culpable for the unwanted outcomes of actions mediated by AI systems
		
	When things go wrong and important interests or rights such as physical integrity or life are infringed on, we, as victims and 4.  Active Responsibility Gap as society, not only want to understand what happened but why. We also want to know whether the harm was the result of someone‚Äôs wrong behaviour, and if it turns out that the  wrong behaviour is one for which there is no justification or excuse, we want the author to be condemned, sanctioned, or 
even punished for their behaviour.
		
	[See p.6; the example of automated driving systems (ADS)]
		
	The use of AI and data-driven machine learning in decision-making importantly introduces a new element of technical opacity and lack of explainability that makes it more difficult for individual persons to satisfy the traditional conditions for moral and legal culpability:  intention, foreseeability, and control.
		
	The more that persons designing, regulating, and operating an AI system can legitimately (and possibly  systematically) avoid blame for their wrong behaviour, the less these (human) agents will be incentivised to prevent these wrong behaviours. In fact, they will arguably have less incentives to strive for a high(er) level of safety, awareness, attention, motivation, and skilfulness.
	
2. Moral Accountability Gap -- AI may make individual persons less able to understand, explain, and reflect upon their own and other (human) agents‚Äô behaviour. Functional society has the expectation that people answer (at least some) why-questions from other persons.
	
	Why-questions do not necessarily have the intention of judging or blaming but possibly to just engage in a conversation and to better understand each other‚Äôs reasons and expectations. [Why were you  late at the appointment, why did you start taking guitar classes, why did you turn down that job  offer...?] This is the core of what it means to be a reflective person in society. In this sense, 4.  Active Responsibility Gap being accountable, unlike being culpable, is something to be desired rather than avoided insofar as it is a constitutive part of being able to reflect on one‚Äôs actions and to participate in meaningful social relations. It also helps  persons see events in the world as connected to their rational capacities and thereby supporting their sense of agency and responsibility.
	
	Moral accountability also has an instrumental value. The process of exchanging questions and reasons helps finding explanations for things that have happened, reinforces trust and social connections between (human) agents, and, by exposing persons to potential requests for explanation and justification, it also tends to reduce undesired behaviour by pushing persons to be more clearly aware of the impact of their actions on others and therefore motivated to prevent unwanted outcomes (and pot4.  Active Responsibility Gap ential culpability)
		
	Moral accountability may be blurred in different ways by the introduction of artificial intelligence. 
		First, in general and similar to what was observed above about culpability, AI contributes to the creation of a more complex chain of decision-making and action (the ‚Äúproblem of many hands‚Äù). AI may make it more difficult for individual agents to make sense  of the reasons why a certain decision was taken, what their role exactly was in the operation, and, in general, whose reasons and what reasoning were governing the system they are part of. 
		Second, data-driven machine (deep) learning, due to its intrinsic opacity, might make a system‚Äôs behaviour extra hard to understand and explain.
==>		Third, the whole process of technology development and production is arguably pervaded by an increasing pressure towards deploying proprietary technologies that, even when working through mechanisms accessible to their developers and programmers, are designed to be inaccessible to public scrutiny and the users themselves. Technology developers, driven by both the desire to minimise industrial espionage and maximise customers loyalty (by, e.g. binding them to their assistance network), will usually avoid sharing data and engineering insights. Thus, there is embedded difficulty for engineers and other (human) agents involved in the process of technological development to systematically discuss with one another their understanding of the goals, meaning, and results of the development process.
	
	[See example p. 9 a medical doctor using an AI-driven system for diagnosing]
	
	Ultimately, the moral accountability gap is the reduction of human agents‚Äô capacity to make sense of ‚Äì and explain to each other (ie to other humans) the ‚Äúlogic‚Äù of their behaviour, due to the mediation (ie the interposition) of opaque, unexplainable AI algorithms and complex autonomous systems and/or the lack of appropriate psychological, social incentives or institutional spaces that promote these explanations.


3. Public Accountability Gap -- Citizens not being able to get an explanation for decisions taken by public agencies, politicians, civil servants, and other human agents invested with a public function.

	Public accountability promotes democratic control (transparency) and limits abuses of power (corruption), but it also brings more effectiveness in the institutions. In fact, providing public administrators with information about their own functioning and forcing them to reflect on their successes and failures will eventually allow and encourage them and others to improve their future performances.

	AI-based decision-making is often difficult to understand and explain to human agents due to the different and sometimes inscrutable ways of AI operations: the so-called ‚Äúblack box‚Äù problem. But the introduction of AI in public administration has also created or aggravated organisational and legal black boxes in governmental institutions.
		First, development in information technologies is often outsourced to private parties or tech-giants, such as Google, who are not politically accountable and may not be willing to disclose critical information about the functioning of their systems. 
		Second, more generally, the work of the software engineers and data professionals in public organisations is usually not visible nor subject to public and legal scrutiny. 
		Finally, far more data are exchanged between many different public organisations than in the past. In  this way, the introduction of AI makes the ‚Äúproblem of many hands‚Äù more acute


4.  Active Responsibility Gap -- the risk that persons designing, using, and interacting with AI may not be sufficiently aware, capable, and/or motivated to see and act according to their moral obligations with respect to the behaviour of the systems they design, control, or use. In particular, this gap concerns the obligation to ensure that AI systems do not negatively impact the rights and interests of other persons and, ideally, ensure that such systems positively contribute to humanity's well-being.


----- Problems with Partial Fixes --------------------------------------- <== 9C, 10A
Common approaches that have so far been taken towards addressing responsibility gaps but that fall short

1. ‚Äúfatalists‚Äù ==> underestimate the extent of the responsibility gap; their focus is too limited.

2. ‚Äúdeflationists‚Äù ==> underestimate the novelty of the AI revolution and its implication for culpability attributions in morality (blameworthiness) and the law (liability); they also seem to underestimate the risks of gaps in the moral and political accountability of system designers as well as gaps in theirs and other (human) agents‚Äô active responsibility for the behaviour of artificial intelligence.

3. ‚Äútechnical solutionism‚Äù ==> Promoters of ‚Äúexplainable AI‚Äù and other scientific and technological improvements tend to ignore the psychological, social, and political dimension of the interaction with AI, thereby running the risk of embracing some form of ‚Äútechnical solutionism‚Äù by which all the moral and social problems of human responsibility for the behaviour of artificial intelligence can be fixed simply by an improvement of the working of AI techniques.

4. ‚Äúlegal solutionism‚Äù ==> Lawyers and policy-makers proposing the revision of current legal liability regimes (including extension of strict and product liability regimes, and ‚Äúelectronic personhood‚Äù) may either underestimate the importance of maintaining some form of human moral responsibility on the behaviour of the artificial intelligence or recognise this need but without saying how moral and social practices ‚Äì and not only legal rules ‚Äì should change in order to govern a responsible transition to the use of AI. 


----- Integrated Solution: Meaningful Human Control ---------
The authors' solution is to promote a legally, ethically, and societally acceptable form of human control over AI systems. Their solution is called Meaningful Human Control (MHS).

MHC requires that two conditions be satisfied: ‚Äútracking‚Äù and ‚Äútracing‚Äù. These two conditions must describe, respectively, i) the nature of the human-machine control relation (alignment with human goals, values, and intentions) and ii) the operational features of the human-machine control relation required to maintain human responsibility on the system (alignment of the system with the capacities of human agents).

Tracking (i above) requires that the socio-technical system ‚Äì i.e. the whole combination of technical, AI, human, and organisational elements ‚Äì is designed to demonstrably respond to the relevant reasons of the relevant agents and to the relevant facts in the environment. Tracking requires the ALIGNMENT of the system with the values, reasons, and intentions of the relevant human agents.

Tracing (ii above) requires that the socio-technical system is designed so that for each of the (relevant) actions of the system, it is possible to identify AT LEAST ONE HUMAN AGENT along the chain of design, development, and use that possesses both (a) sufficient knowledge of the capabilities and limitations of the system and (b) sufficient moral awareness of, and capacity to comply with, her role as potential target of legitimate response for the behaviour of the system. Tracing requires the align4.  Active Responsibility Gap ment of the system with the capacities of the relevant human agents. Tracing is an operational, causal notion of control ‚Äì mainly adopted in scientific and technical domains ‚Äì such that a technical system is under the control of a human agent when there is a reliable causal connection between the human agent and the machine‚Äôs behaviour.


** Final Exam 


** More Murphy's English Grammar in Use
	Unit			Pg
	142				284		Phrasal verbs 6 up / down



** Bless Me, Ultima
Chapter 8

2. Moral Accountability Gap -- AI may make individual persons less able to understand, explain, and reflect upon their own and other (human) agents‚Äô behaviour. Functional society has the expectation that people answer (at least some) why-questions from other persons.
Chapter 9


Chapter 10				<== Start here with 10A



Chapter 11
	Narciso's garden (p 109)
	Is there a bruja at your house?" Ernie asked. (p 110)
	The Golden Carp (p 113)
	"The mermaid?" I questioned him. (p 115)
	"This whole town is sitting over a deep, underground lake! (p 118)
	The Prophecy of the Golden Carp
	You have been seeing only parts, she finished, and not looking beyond into the great cycle that binds us all. (p 121)
	
Chapter 12									
	the three dolls on her shelf. (p 123)
	She took her scapular (p 124)
	The Good Old Days 
	the huge figure of Narciso, the warning, the accusation, Tenorio, the Owl
	the Test (p 126-135)


Chapter 13									<== 9B start here
	The trip to El Puerto w/ Pedro (p 137)
	The witch's procession and rejection
	Lunas are happy with Antonio
	
	
Chapter 14
	Back to school with the Gang of crazies (p 145)
	Christmas and the school play (p 151 - 158)
	The fight (p 160)
	Narciso to warn Ultima, Andrew at Rosie's (p 163)
	["I had seen evil, and so I carried the evil within me"]
	Tenorio kills Narciso (p 168-170)
	The Pesadilla (p 172)
	
	
Chapter 15
	Tenorio unpunsished; few people remembered anything good about Narciso (p 177)
	Ultima worked so hard to try to save his young wife (p 179)
	Leon and Eugene return for a visit (in a cop car!)
	Gabriel and his resentment (p 183)
	Andrew leaves (p 185)
	



--- Coming up

** Marron's Tips for Consulting
** 3P exam 
** Final Exam




---- Work time
	









